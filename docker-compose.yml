services:
  server:
    build:
      context: ./server
      dockerfile: Dockerfile
    volumes:
      - server_sqlite_data:/app/data # Persist the SQLite database between container restarts
    expose:
      - "3000" # Expose port 3000 to other containers in the same network
    environment:
      - SCRAPER_SECRET=${SCRAPER_SECRET:-your_very_secret_string_here}
      - SEQUELIZE_LOGGING=false
      - SQLITE_STORAGE=/app/data/Database.sqlite
      - ANALYTICS_STORAGE_ITEMS=200
      - ANALYTICS_STORAGE_TIME_MINUTES=10
      - ANALYTICS_HASH_SALT=${ANALYTICS_HASH_SALT:-your_very_secret_string_here}
      - CINESTAR_API_URL=${CINESTAR_API_URL:-https://shop.cinestarcinemas.hr/api}
      - PUPPETEER_MAX_CONCURRENCY=${PUPPETEER_MAX_CONCURRENCY:-2}
      - PUPPETEER_IDLE_TTL_MS=${PUPPETEER_IDLE_TTL_MS:-1200000}
      - PUPPETEER_NAV_TIMEOUT_MS=${PUPPETEER_NAV_TIMEOUT_MS:-12000}
      - PUPPETEER_USER_AGENT=${PUPPETEER_USER_AGENT:-}
      - PUPPETEER_EXECUTABLE_PATH=${PUPPETEER_EXECUTABLE_PATH:-}
      - SEATING_CACHE_TTL_MS=${SEATING_CACHE_TTL_MS:-60000}
      - SEATING_CACHE_MAX_ENTRIES=${SEATING_CACHE_MAX_ENTRIES:-50}
      # Optional: DB schema control (increment or set FORCE_DB_RESET=true to wipe DB on startup)
      - DB_SCHEMA_VERSION=${DB_SCHEMA_VERSION:-1}
      - FORCE_DB_RESET=${FORCE_DB_RESET:-false}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 1m30s # Time between health checks
      timeout: 30s # Time to wait for a health check to succeed
      retries: 5 # Number of retries before considering the service unhealthy
      start_period: 15s # Time to wait before starting health checks

  client:
    build:
      context: ./client
      dockerfile: Dockerfile
      args:
        PUBLIC_API_URL: "https://api.bettercinestar.com"
    # ports:
    #   - "8080:80" # Map port 8080 on the host to port 80 in the container
    depends_on:
      - server
    restart: unless-stopped

  # Scraper service (runs once and exits)
  scraper:
    build:
      context: ./scraper
      dockerfile: Dockerfile
    environment:
      - SERVER_API_URL=http://server:3000/api/v1/scrape-results
      - SCRAPER_SECRET=${SCRAPER_SECRET:-your_very_secret_string_here}
      - TMDB_API_KEY=${TMDB_API_KEY:-}
      # Set RUN_MODE to 'once' to run the scraper once and exit, or 'scheduled' to run on a schedule
      - RUN_MODE=scheduled
      # Set the cron schedule for periodic runs (only used if RUN_MODE=scheduled)
      - CRON_SCHEDULE=0 2 * * * # Every day at 2 AM
      - SCRAPE_ON_START=${SCRAPE_ON_START:-true}
    depends_on:
      - server
    restart: unless-stopped

volumes:
  server_sqlite_data:
